# -*- coding: utf-8 -*-
"""toce_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1huoymw63sD5ranytGkARdwtkfFY2bgaZ
"""

#@title Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#@title Get Data

toce_data = pd.read_csv("https://raw.githubusercontent.com/fares-ipgp/toce/main/data/csv/toc_data.csv")
df = toce_data
df

#@title Select features
#@markdown Numerical values. Impute NaN as mean.
# bioturb	amb1	amb2
feature_names = []

columns_geological = [
                    #'depth_m',
                   	'facies_num',
                    'bioturb',
                   	'amb1',
                   	'amb2',
                    #'drx_qz',
                   	#'drx_pl',
                   	#'drx_fk',
                   	#'drx_ca',
                   	#'drx_py',
                   	#'drx_arc',
                   	#'drx_arc_i',
                   	#'drx_arc_is',
                   	#'drx_arc_cl',
                   	#'drx_arc_k'
                   ]

columns_analysis = [
                    #'depth_m',
                   	#'facies_num',
                    #'bioturb',
                   	#'amb1',
                   	#'amb2',
                    'drx_qz',
                   	'drx_pl',
                   	'drx_fk',
                   	'drx_ca',
                   	'drx_py',
                   	'drx_arc',
                   	'drx_arc_i',
                   	'drx_arc_is',
                   	'drx_arc_cl',
                   	'drx_arc_k'
                   ]

columns_numeric = [
                    #'depth_m',
                   	'facies_num',
                    'bioturb',
                   	'amb1',
                   	'amb2',
                    'drx_qz',
                   	'drx_pl',
                   	'drx_fk',
                   	'drx_ca',
                   	'drx_py',
                   	'drx_arc',
                   	'drx_arc_i',
                   	'drx_arc_is',
                   	'drx_arc_cl',
                   	'drx_arc_k'
                   ]

# Numeric feature names
feature_names.extend(columns_numeric)

# Mean imputer
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
df[columns_numeric] = imp.fit_transform(df[columns_numeric])
df

#@title Target { run: "auto" }

columns_target = ['toc','if']
columns_label = ['sample']

#@title Preprocessing  
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

def build_pre_cols_num(cols):
  '''
  Builds column processors
  '''
  num = StandardScaler()
  pre_cols = ColumnTransformer(
      transformers=[
          ('num', num, columns_numeric),
          ])

  return pre_cols, columns_numeric

# Build column processor
pre_cols_num, _ = build_pre_cols_num(columns_numeric)

#@title TSNE TOC*IF - All features { run: "auto" }

from sklearn.manifold import TSNE

X = df[columns_numeric]
y = df[columns_target]

tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=10000,learning_rate=0.01,early_exaggeration=1,n_iter_without_progress=200, metric='minkowski')
tsne_results = tsne.fit_transform(X)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
current_cmap = plt.cm.get_cmap()
current_cmap.set_bad(color='red')

im=ax.scatter(tsne_results[:,0],tsne_results[:,1],c='red', marker='x', alpha=0.2)
im=ax.scatter(tsne_results[:,0],tsne_results[:,1],cmap=current_cmap, c=y['toc']*y['if'],s=100)
cbar=fig.colorbar(im, ax=ax)
cbar.set_label('TOC*IF')

plt.title("T-SNE\nFeatures: \n"+str(columns_numeric))

for i, txt in enumerate(df['sample']):
     ax.annotate(txt  , (tsne_results[i,0], tsne_results[i,1]))

#@title TSNE TOC*IF - Geological features { run: "auto" }

from sklearn.manifold import TSNE

X = df[columns_geological]
y = df[columns_target]

tsne = TSNE(n_components=2, verbose=1, perplexity=10, n_iter=10000,learning_rate=0.01,early_exaggeration=1,n_iter_without_progress=200, metric='minkowski')
tsne_results = tsne.fit_transform(X)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
current_cmap = plt.cm.get_cmap()
current_cmap.set_bad(color='red')

im=ax.scatter(tsne_results[:,0],tsne_results[:,1],c='red', marker='x', alpha=0.2)
im=ax.scatter(tsne_results[:,0],tsne_results[:,1],cmap=current_cmap, c=y['toc']*y['if'],s=100)
cbar=fig.colorbar(im, ax=ax)
cbar.set_label('TOC*IF')

plt.title("T-SNE\nFeatures: \n"+str(columns_geological))

for i, txt in enumerate(df['sample']):
     ax.annotate(txt  , (tsne_results[i,0], tsne_results[i,1]))

#@title TSNE TOC*IF - Analysis features { run: "auto" }

from sklearn.manifold import TSNE

X = df[columns_analysis]
y = df[columns_target]

tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=10000,learning_rate=0.01,early_exaggeration=1,n_iter_without_progress=200, metric='minkowski')
tsne_results = tsne.fit_transform(X)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
current_cmap = plt.cm.get_cmap()
current_cmap.set_bad(color='red')

im=ax.scatter(tsne_results[:,0],tsne_results[:,1],c='red', marker='x', alpha=0.2)
im=ax.scatter(tsne_results[:,0],tsne_results[:,1],cmap=current_cmap, c=y['toc']*y['if'],s=100)
cbar=fig.colorbar(im, ax=ax)
cbar.set_label('TOC*IF')

plt.title("T-SNE\nFeatures: \n"+str(columns_analysis))

for i, txt in enumerate(df['sample']):
     ax.annotate(txt  , (tsne_results[i,0], tsne_results[i,1]))

#@title Metric Functions { run: "auto" }

#Validation
from sklearn.model_selection import KFold

#Metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_curve

# Plot
import seaborn as sns

def df_merge_shuffle(dfs: list, shuffle=True) -> pd.DataFrame:
  '''
  Given a list of datasets, merge, shuffle and reindex them
  '''
  df_out = None

  for idx, df in enumerate(dfs):
    if idx == 0:
      df_out = df
    else:
      df_out = pd.concat([df_out, df], axis=0)
  
  if shuffle:
    df_out = df_out.sample(frac=1).reset_index(drop=True)
  return df_out 


def calc_metrics(y_true, y_pred) -> dict:
  '''
  Evaluate metrics and return them in a dictionary
  '''
  metrics = {}
  metrics['accuracy']=accuracy_score(y_true,y_pred)
  metrics['f1_score']=f1_score(y_true,y_pred)
  metrics['precision']=precision_score(y_true,y_pred)
  metrics['recall']=recall_score(y_true,y_pred)

  return metrics

def eval_metrics(clf, X_train, y_train, X_val, y_val ) -> dict:
  
  # Trining metrics
  y_pred = clf.predict(X_train)
  train_metrics = calc_metrics(y_train,y_pred)
    
  # Convert to dataset
  df_metrics_train = pd.DataFrame.from_dict([train_metrics], orient='columns')
  df_metrics_train['data'] = 'train'

  # Validation metrics
  y_pred = clf.predict(X_val)
  val_metrics = calc_metrics(y_val,y_pred)
  
  # Convert to dataset
  df_metrics_val = pd.DataFrame.from_dict([val_metrics], orient='columns')
  df_metrics_val['data'] = 'val'

  # Merge data
  df_metrics = df_merge_shuffle([df_metrics_train, df_metrics_val])

  return df_metrics

def eval_kfold(clf, X, y, k, fit=True):

  # Create KFold splitter
  kf = KFold(n_splits=k,shuffle=True)

  # Init dictionaries to store results
  metrics=[]

  # Iterate over splits
  for split, indexes in enumerate(kf.split(X)):

    # Unpack indexes
    train_index, val_index = indexes 
    
    # Split dataset
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Fit model
    if fit:
      clf.fit(X_train, y_train)

    # Calculate metrics
    df_m = eval_metrics(clf, X_train, y_train , X_val, y_val)
    df_m['split']=split
    metrics.append(df_m)

  return  df_merge_shuffle(metrics,shuffle=False)

def plot_metrics(df_metrics,ax = None):
  '''
  Plots a dataframe of metrics
  '''  
  if ax is None: 
    ax = plt.gca()

  sns.boxplot(x=df_metrics.metric, y=df_metrics.value, hue=df_metrics.data,palette="colorblind")

#@title Prepare data for training / testing { run: "auto" }

# Feature column names
columns_features = columns_numeric 

# split in labeled/unlabeled 
df_labeled=df.dropna()
df_unlabeled=df[df.isna().any(axis=1)]

labels=df_labeled[columns_label].reset_index()
X = df_labeled[columns_features].reset_index()
y = df_labeled[columns_target].reset_index()

#@title Classifier { run: "auto" }

# Classifiers
from sklearn import neighbors 
from sklearn import ensemble
from sklearn import svm

# Feature column names
columns_features = columns_numeric 

# split in labeled/unlabeled 
df_labeled=df.dropna()

labels=df_labeled[columns_label].reset_index()
X = df_labeled[columns_features].reset_index()
y = df_labeled[columns_target].reset_index()

# Create classifier
knn=10
clf = neighbors.KNeighborsClassifier(knn, weights='uniform',metric='minkowski')
clf = ensemble.RandomForestClassifier(1000,max_depth=10,min_samples_split=10,min_samples_leaf=5)

# Evaluate metrics
k_fold=3
metrics = eval_kfold(clf,pre_cols_num.fit_transform(X), y['toc']>1, k_fold)

# Convert from wide to long format
metrics_long = metrics.melt(id_vars=["data", "split"], 
        var_name="metric", 
        value_name="value")

# Plot result
fig = plt.gcf()
fig.set_size_inches(8,8)
ax = plt.gca()
ax.set_title(f'{knn:d}-KNN Cross validation metrics - k_fold:{k_fold:d}')
plot_metrics(metrics_long)

#@title Data augmentation { run: "auto" }

# split in labeled/unlabeled 
df_labeled=df.dropna()
df_unlabeled=df[df.isna().any(axis=1)]

labels=df_labeled[columns_label].reset_index()
X = df_labeled[columns_features].reset_index()
y = df_labeled[columns_target].reset_index()
X_unlabeled = df_unlabeled[columns_numeric]

k= 3

# Create KFold splitter
kf = KFold(n_splits=k,shuffle=True)

X = df_labeled.reset_index()[columns_features]
y = df_labeled.reset_index()['toc']>1
X_unlabeled = df_unlabeled[columns_numeric].reset_index()

# Init dictionaries to store results
metrics=[]

# Iterate over splits
for split, indexes in enumerate(kf.split(X)):

  # Unpack indexes
  train_index, val_index = indexes 

  # Split dataset
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  # Fit model 1
  clf1 = neighbors.KNeighborsClassifier(1, weights='uniform',p=2)
  clf1.fit(pre_cols_num.fit_transform(X_train), y_train)

  # Predict targets for unlabeled 
  y_augm=clf1.predict(pre_cols_num.fit_transform(X_unlabeled))

  # Fit model 2
  clf2 = ensemble.RandomForestClassifier(1000,max_depth=10,min_samples_split=10,min_samples_leaf=5)
  clf2.fit(pre_cols_num.fit_transform(X_unlabeled),y_augm)

  # Calculate metrics for model 2
  df_m = eval_metrics(clf2, pre_cols_num.fit_transform(X_train), y_train , pre_cols_num.fit_transform(X_val), y_val)
  df_m['split']=split
  metrics.append(df_m)

metrics= df_merge_shuffle(metrics,shuffle=False)

# Convert from wide to long format
metrics_long = metrics.melt(id_vars=["data", "split"], 
        var_name="metric", 
        value_name="value")

# Plot result
fig = plt.gcf()
fig.set_size_inches(8,8)
ax = plt.gca()
ax.set_title(f'{knn:d}-KNN Cross validation metrics - k_fold:{k_fold:d}')
plot_metrics(metrics_long)